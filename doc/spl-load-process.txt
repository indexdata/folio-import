Spokane Public Library data conversion and loading process

Data files are located in the hz-okapi project under the assets directory.
These files come in different formats: .mrc .json and a Horizon flavor of
JSON that is usual broken.

1.  Load all settings (ref data) if needed.  Currently locations, service-points, etc.
    get saved off as a previous system's settings and loaded into a new system.

2.  Instances (and SRS):

    a.  Concatenate the group*.mrc files into a single large file.
            ex: cat group*.mrc >> large-file.mrc

    b.  Convert large marc file to utf8.
            ex: yaz-marcdump -f marc8 -t utf8 -o marc large-file.mrc > utf8.mrc

    NOTE: The marc files do not have the record ID in the 001 but mostly the 035.
    We need to run a script to "fix" this file by moving the record ID to the 001.

    c.  Fix the utf8 encoded marc file by running /perl/marc_ctrl_fix.pl

    d.  Download inventory reference data by running ./reference_inventory.sh (in /bash)

    e.  Run /perl/marc2inst.pl to create instances and SRS records.
            You will need to have a local copy of the FOLIO mapping rules
            and inventory reference data to run the above script. The script
            creates JSONL files for instances and SRS.  It will also create a
            snapshot object and instances map file (previous system ID to FOLIO uuid)

    f.  Use loadInventorySyncJSONL.js to load instances.
            ex: node loadInventorySyncJSONL.js data/instances.jsonl

    g.  Run loadJSONL.js to add snapshot.jsonl.
            ex: node loadJSONL.js source-storage/snapshots data/snapshot.jsonl

    h.  Load SRS objects.
            ex: node loadJSONL.js source-storage/records data/srs.jsonl

    NOTE: When loading instances and SRS, it may be a good idea to run several processes
    at the same time.  Use the Unix split command on the jsonl file.  This will create many 
    smaller files.  There is a script called run_inventory.sh that will run a
    background process for every file in a glob.
            ex: ./run_inventory.sh data/xa*

    Likewise, there is the run_load_jsonl.sh script that will load several jsonl files as a
    background process.

2. Holdings and items:

    a.  As above, concatenate the hz-holdings-*.json files into one large file.

    b.  There needs to be a collection.tsv file located in the same directory as
    as the reference data (previously downloaded).  There is a collections.json file
    located in the assets directory.  This file needs to be converted to TSV first by
    using jq.

        ex: jq -r '.mtypes[] | .code + "\t" + .name' collections.json > /refdir/collections.tsv

    c.  Run /perl/items_spl.pl to convert the Horizon holdings to Folio holdings and items.
            ex: ./items_spl.pl /refdir/ large-holdings.json file.

    d.  Load with loadInventorySyncStream.js (or convert the holdings and items files to
        JSONL format and use the multi-process runner ./run_inventory.sh).
        NOTE: The holdings and items files may be split using the splitLargeJson.js script

3.  Users.

    a.  Concatenate hz-users-*.json files to create one large users file.

    b.  Run splUsers.js script (located in the /patron directory).  This process
        will create users files in addition to permissions and logins collections.

    c.  Now run loadByEndpoint.js on users.json files.  Perhaps all of these users files
        could be concatenated into several larger ones and the use the run_load_endpoint.sh 
        to run 5 or 10 jobs in the background.  NOTE: The reason why so many users files get
        created is because I used to use mod-user-import.  There were some issues with this
        module in the past, so now splUsers.js now creates FOLIO users objects instead.