Spokane Public Library data conversion and loading process

Data files are located in the hz-okapi project under the assets directory.
These files come in different formats: .mrc .json and a Horizon flavor of
JSON that is usual broken.

Some notes about use of [screen](https://github.com/indexdata/id-folio-infrastructure/blob/master/runbooks/screen.md) 

0.  Load all settings (ref data) if needed.  Currently locations, service-points, etc.
    get saved off as a previous system's settings and loaded into a new system.

    a. Download the reference data from the current system.
       cd ~/folio-import
       # ensure appropriate config.js e.g. spokane-test
       node downloadAllRefData.js /data/spl/ref/

    b. Adjust Makefile config. Ensure appropriate login details, for test or production:
       ~/folio-import/config.js and ~/folio-import/bash/login-spokane.sh 
       cp ~/folio-import/Makefile-spl /data/spl/Makefile  # and adjust the "ten" variable

    c. Remove the default reference data.
       This is done with Makefile 'make remove-default-ref'

    d. Load reference data.
       This is done with Makefile early sections up to 'templates'

    e. Refresh the source data.
       cd /data/spl/hz-okapi
       git pull

    f. Configure workspace.
       cd ~
       ln -s /data/spl spl
       cd ~/folio-import
       cp configs/js/spokane.js config.js
       # in config.js use: logpath: '../spl/log'
       cp configs/bash/login-spokane.sh bash
       cd bash
       ln -s /data/spl/log log

1.  Instances (and SRS):

    Steps a-c are done via Makefile 'make marc'
    Steps d-e are done via Makefile 'make instances'

    a.  Concatenate the group*.mrc files into a single large file.
            ex: cat group*.mrc >> large-file.mrc

    b.  Convert large marc file to utf8.
            ex: yaz-marcdump -f marc8 -t utf8 -o marc -l 9=97 large-file.mrc > utf8.mrc

    NOTE: The marc files do not have the record ID in the 001 but mostly the 035.
    We need to run a script to "fix" this file by moving the record ID to the 001.

    c.  Fix the utf8 encoded marc file by running /perl/marc_ctrl_fix.pl

    d.  Download inventory reference data and folio mapping rules.
        cd ~/folio-import/bash
        ./reference_inventory.sh /data/spl/inv/ref
        ./get_mapping_rules.sh > /data/spl/inv/ref/source_record_manager_module/mapping-rules.json

    e.  Run /perl/marc2inst.pl to create instances and SRS records.
            The script creates JSONL files for instances and SRS.  It will also create a
            snapshot object and instances map file (previous system ID to FOLIO uuid)

    f.  Use loadInventorySyncJSONL.js to load instances.
            ex: node loadInventorySyncJSONL.js data/instances.jsonl

    g.  Run loadJSONL.js to add snapshot.jsonl.
            ex: node loadJSONL.js source-storage/snapshots data/snapshot.jsonl

    h.  Load SRS objects.
            ex: node loadJSONL.js source-storage/records data/srs.jsonl

    NOTE: When loading instances and SRS, it may be a good idea to run several processes
    at the same time.  Use the Unix split command on the jsonl file.  This will create many 
    smaller files.  There is a script called run_inventory.sh that will run a
    background process for every file in a glob.
            ex: ./run_inventory.sh data/xa*

    Likewise, there is the run_load_jsonl.sh script that will load several jsonl files as a
    background process.

2.  Holdings and items:

    Steps a-c are done via Makefile 'make holdings-items'

    a.  As above, concatenate the hz-holdings-*.json files into one large file.

    b.  There needs to be a collections.tsv file located in the same directory as
    as the reference data (previously downloaded).  There is a collections.json file
    located in the assets directory.  This file needs to be converted to TSV first by
    using jq.

        ex: jq -r '.mtypes[] | .code + "\t" + .name' collections.json > /refdir/collections.tsv

    c.  Run /perl/items_spl.pl to convert the Horizon holdings to Folio holdings and items.
            ex: ./items_spl.pl /refdir/ large-holdings.json file.

    d.  Load with loadInventorySyncStream.js (or convert the holdings and items files to
        JSONL format and use the multi-process runner ./run_inventory.sh).
        NOTE: The holdings and items files may be split using the splitLargeJson.js script

3.  Users:

    Steps a-c are done via Makefile 'make users'
    Step d is done with Makefile 'make users'
    
    a.  Concatenate hz-users-*.json files to create one large users file.
    
    b.  Get groups.json from FOLIO and save it to the same directory as the large hz-users file.
            ex:  node downloadAllByEndpoint.js groups ../spl_data/users/

    c.  Run splUsers.js script (located in the /patrons directory).  This process
        will create a file of users objects called users_00001.json in addition to permissions and logins.

    d.  Run node splitLargeJson.js to create multiple files.
    
    e.  Run ./run_load_jsonl.sh users users*jsonl to load users.
    
    f.  After users are loaded, run ./run_load_jsonl.sh perms*jsonl
    
    g.  While perms are loading, feel free to run ./run_load_jsonl.sh logins*jsonl
    
4.  Loans:

    Steps a-d are done via Makefile 'make loans'
    Step e is done via Makefile 'make toggle-inactive-users'
    Then do step f, as instructed
    Step g is done via Makefile 'make toggle-inactive-users'

    a.  Loans data from Spokane are located in the assets directory as hz-loans-(whatever).  Concatenate
        all loans objects into one loans.json files using jq.

        ex: jq '.[]' ../hz-okapi/assets/json/hz-loans-\(* | jq -s . > hz-loans.json

    b.  First download all users from FOLIO using the downloadAllUsers.js script.
        
        ex: node downloadAllUsers.js <working_directory>

    c.  Get service points with downloadAllByEndpoint.js

        ex: node downloadAllByEndpoint.js service-points <working_directory>

    d.  Run createSplCheckout.js in the /patrons/ directory.

        ex: node splCheckouts.js <working_dir>/service-points.json <working_dir>/users.json <working_dir>/spokane/circ/loans.json

        The output from this script will create three files:
        
                * checkouts.json (load this file)
                * inactive_checkouts.json (user for active toggle)
                * notfound_checkouts.json (do not load-- users are not found for these checkouts)

    e.  The checkouts.json file contains loans for active and inactive users.  FOLIO doesn't allow checkouts to
        to inactive users, so these need to be made temporarily active.  Run usersActiveToggle.js to do this.

        ex: node usersActiveToggle.js <working_dir>/inactive_checkouts.json

    f.  Use the checkoutByBarcode.js script on the checkouts.json file.

        ex: node checkoutByBarcode.js <working_dir>/checkouts.json

    g.  Run usersActiveToggle.js again to reset inactive users to inactive.

5.  Requests:

    Requests are stored in the /assets/ directory as hz-requests.json.

    a.  Run splRequests.js to create FOLIO requests objects.

        ex: node splRequests <working_dir>/hz-requests.json

        This script will contact FOLIO to check the status of certain items.  It will also
        try to place requests on items in a round robin fashion, first checking for available items,
        then items with the least amount of holds.  This process will load the request objects
        into FOLIO instead of creating an objects file.

        NOTE: Do not run multiple processes while during this step.

6.  Fines:

    Steps a-d are done via Makefile 'make feesfines-assess' to obtain data and do some basic verification.
    Step e is done via Makefile 'make feesfines-transform'
    Steps f-j are manual steps to load the various data.

    a.  Get copy of some supporting data:

        cp ~/folio-import/python/spl-feesfines/verbs-map.tsv /data/spl/feesfines
        cp /data/dcrossley/spl-feesfines/uuids-ff*.json /data/spl/feesfines

    b.  Get some reference data:

        cd ~/folio-import
        # ensure appropriate config.js
        node downloadAllByEndpoint.js locations /data/spl/feesfines
        node downloadAllByEndpoint.js material-types /data/spl/feesfines
        node downloadAllByEndpoint.js service-points /data/spl/feesfines

    c.  Get map of users:

        node downloadAllUsers.js /data/spl/feesfines
        cat /data/spl/feesfines/users.json \
          | jq -r '.users[] | select(has("externalSystemId")) | [ .externalSystemId, .id ] | @tsv' \
          > /data/spl/feesfines/users-map.tsv

    d.  Get the item data:

        node downloadAllItems.js /data/spl/feesfines

    e.  Run the transformation:

        cd /data/spl/feesfines
        python3 ~/folio-import/python/spl-feesfines/spl_feesfines.py \
          -u users-map.tsv -m items.json -v verbs-map.tsv \
          -c locations.json -y material-types.json -p service-points.json \
          -l debug \
          -i /data/spl/hz-okapi/assets/json/hz-ledger.json

    f.  Inspect the summary and errors output:

        ex: cat errors-feesfines.json | jq '.[] | select(.errors[] | match("uuid_user:")) | .borrower' | sort -u -n
        ex: cat errors-feesfines.json | jq '.[].errors[]' | egrep -v 'uuid_user: missing|uuid_item: missing' | sort -u

    g.  Load the accounts:

        cd ~/folio-import
        node loadJSONL.js accounts /data/spl/feesfines/feesfines-accounts.jsonl
        tail -4 /data/spl/log/feesfines-accounts.jsonl.log
        # Inspect log for errors. Should only be from potential network glitches.
        # Isolate records to new data file, and load those missing ones.

    h.  Load the associated actions:

        node loadJSONL.js feefineactions /data/spl/feesfines/feesfines-actions.jsonl
        tail -4 /data/spl/log/feesfines-actions.jsonl.log
        # Inspect log for errors. As above.

    i.  Load the adjustment actions:

        cd bash
        ./login-spokane.sh  # or login-spokane-test.sh
        ./load_feesfines_adjustments.sh /data/spl/feesfines/feesfines-adjustments.jsonl
        # Inspect log for errors.
        # Do not re-load any, as balances are calculated.

    j.  Load the transactions:

        ./load_feesfines_transactions.sh /data/spl/feesfines/feesfines-transactions.jsonl
        # Inspect log for errors.
        # Do not re-load any, as balances are calculated.

7.  Organizations: (TODO)

    There is an old file in /assets/ named organizations-2020-12-01.json.  This looks Like
    a collection of FOLIO organization objects.  I don't know if there is anything to do except load?

8.  Orders: (TODO)

    There is an orders.py script here: https://gitlab.com/spokanepubliclibrary/hz-okapi/-/blob/master/scripts/orders.py 
    Perhaps this does something?

9.  Invoices (TODO)


     

    
